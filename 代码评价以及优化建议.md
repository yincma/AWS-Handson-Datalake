# AWSæ•°æ®æ¹–é¡¹ç›®ä»£ç è¯„ä»·ä»¥åŠä¼˜åŒ–å»ºè®®

## æ‰§è¡Œæ‘˜è¦

ä½œä¸ºèµ„æ·±æ¶æ„å¸ˆï¼Œæˆ‘å¯¹è¿™ä¸ªAWSæ•°æ®æ¹–é¡¹ç›®è¿›è¡Œäº†å…¨é¢çš„ä»£ç è´¨é‡è¯„ä¼°ã€‚è¯¥é¡¹ç›®å±•ç°äº†è‰¯å¥½çš„æ•´ä½“æ¶æ„è®¾è®¡å’Œå®ç”¨æ€§ï¼Œä½†åœ¨ä»£ç ç»„ç»‡ã€æ¨¡å—åŒ–è®¾è®¡ã€é”™è¯¯å¤„ç†å’Œå¯ç»´æŠ¤æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚

**æ€»ä½“è¯„åˆ†**: 7.2/10
- **æ¶æ„è®¾è®¡**: 8.5/10 â­â­â­â­â­ 
- **ä»£ç è´¨é‡**: 6.5/10 â­â­â­
- **æ¨¡å—åŒ–è®¾è®¡**: 5.8/10 â­â­â­
- **å®‰å…¨æ€§å®ç°**: 8.0/10 â­â­â­â­
- **å¯ç»´æŠ¤æ€§**: 6.0/10 â­â­â­
- **æ–‡æ¡£è´¨é‡**: 9.0/10 â­â­â­â­â­

---

## 1. æ¶æ„è®¾è®¡è¯„ä»·

### ğŸ† ä¼˜ç‚¹

**ä¸‰å±‚æ•°æ®æ¹–æ¶æ„**
- æ¸…æ™°çš„åˆ†å±‚è®¾è®¡ï¼ˆRaw â†’ Clean â†’ Analyticsï¼‰
- ç¬¦åˆç°ä»£æ•°æ®æ¹–æœ€ä½³å®è·µ
- æ”¯æŒæ•°æ®è¡€ç¼˜å’Œæ²»ç†

**æœåŠ¡é€‰æ‹©åˆç†**
- AWSæœåŠ¡é›†æˆåº¦é«˜ï¼ˆS3ã€Glueã€EMRã€Athenaã€Lake Formationï¼‰
- Infrastructure as Codeå®ç°å®Œæ•´
- æˆæœ¬ä¼˜åŒ–è€ƒè™‘å……åˆ†

**éƒ¨ç½²è‡ªåŠ¨åŒ–**
- ä¸€é”®éƒ¨ç½²åŠŸèƒ½å¼ºå¤§
- æ”¯æŒå¤šç¯å¢ƒé…ç½®
- æ•…éšœæ¢å¤æœºåˆ¶å®Œå–„

### âš ï¸ æ”¹è¿›ç‚¹

**ç¼ºä¹å¾®æœåŠ¡åŒ–è®¾è®¡**
- å•ä½“è„šæœ¬è¾ƒå¤šï¼ŒåŠŸèƒ½è€¦åˆåº¦é«˜
- ç¼ºä¹æœåŠ¡è¾¹ç•Œæ¸…æ™°çš„æ¨¡å—åˆ’åˆ†
- éš¾ä»¥æ”¯æŒå¤§è§„æ¨¡å›¢é˜Ÿåä½œ

---

## 2. ä»£ç è´¨é‡è¯¦ç»†åˆ†æ

### 2.1 Bashè„šæœ¬è´¨é‡è¯„ä¼°

#### ğŸ”´ ä¸»è¦é—®é¢˜

**å‡½æ•°é‡å¤å’Œä»£ç å†—ä½™**
```bash
# é—®é¢˜ï¼šå¤šä¸ªè„šæœ¬ä¸­é‡å¤çš„æ—¥å¿—å‡½æ•°å®šä¹‰
print_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }
```
**ä½ç½®**: `deploy-all.sh`, `setup-env.sh`, `create-emr-cluster.sh`, `cleanup.sh`

**é”™è¯¯å¤„ç†ä¸ä¸€è‡´**
- éƒ¨åˆ†è„šæœ¬ä½¿ç”¨ `set -e`ï¼Œéƒ¨åˆ†ä¸ä½¿ç”¨
- é”™è¯¯æ¢å¤æœºåˆ¶ç¼ºå¤±
- å…³é”®æ“ä½œç¼ºä¹å¹‚ç­‰æ€§è®¾è®¡

**ç¡¬ç¼–ç é—®é¢˜**
```bash
# é—®é¢˜ï¼šé­”æ³•æ•°å­—å’Œç¡¬ç¼–ç é…ç½®
EMR_INSTANCE_TYPE=m5.xlarge    # åº”è¯¥é…ç½®åŒ–
INSTANCE_COUNT=3               # ç¼ºä¹éªŒè¯é€»è¾‘
```

#### ğŸŸ¢ ä¼˜åŒ–å»ºè®®

**1. åˆ›å»ºé€šç”¨å·¥å…·åº“**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/lib/common.sh
#!/bin/bash
# ç»Ÿä¸€çš„å·¥å…·å‡½æ•°åº“

source_common_functions() {
    # ç»Ÿä¸€æ—¥å¿—å‡½æ•°
    print_info() { 
        echo -e "${GREEN}[INFO]${NC} $(date '+%H:%M:%S') $1" 
        logger -t "datalake" "INFO: $1"
    }
    
    # ç»Ÿä¸€é”™è¯¯å¤„ç†
    handle_error() {
        local exit_code=$1
        local error_message="$2"
        print_error "$error_message"
        cleanup_on_error
        exit $exit_code
    }
    
    # ç»Ÿä¸€å‰ç½®æ£€æŸ¥
    validate_prerequisites() {
        check_aws_cli_version
        validate_aws_permissions
        verify_required_env_vars
    }
}
```

**2. å®ç°é…ç½®éªŒè¯æœºåˆ¶**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/lib/config-validator.sh
validate_configuration() {
    local config_file="$1"
    
    # éªŒè¯å¿…éœ€å‚æ•°
    required_vars=("AWS_REGION" "PROJECT_PREFIX" "ENVIRONMENT")
    for var in "${required_vars[@]}"; do
        [[ -z "${!var}" ]] && handle_error 1 "Missing required variable: $var"
    done
    
    # éªŒè¯å‚æ•°æ ¼å¼
    [[ ! "$AWS_REGION" =~ ^[a-z]{2}-[a-z]+-[0-9]$ ]] && \
        handle_error 1 "Invalid AWS region format: $AWS_REGION"
    
    # éªŒè¯èµ„æºé™åˆ¶
    validate_resource_quotas
}
```

**3. å¢å¼ºé”™è¯¯æ¢å¤èƒ½åŠ›**
```bash
# å»ºè®®å®ç°ï¼šæ™ºèƒ½é‡è¯•æœºåˆ¶
retry_with_backoff() {
    local max_attempts=3
    local delay=1
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if "$@"; then
            return 0
        else
            if [ $attempt -lt $max_attempts ]; then
                print_warning "Attempt $attempt failed, retrying in ${delay}s..."
                sleep $delay
                delay=$((delay * 2))
            fi
            attempt=$((attempt + 1))
        fi
    done
    
    return 1
}
```

### 2.2 Pythonä»£ç è´¨é‡è¯„ä¼°

#### ğŸ”´ ä¸»è¦é—®é¢˜

**ç¼ºä¹é¢å‘å¯¹è±¡è®¾è®¡**
```python
# é—®é¢˜ï¼šè¿‡ç¨‹å¼ç¼–ç¨‹ï¼Œç¼ºä¹æŠ½è±¡
def create_glue_tables():
    # 500+ è¡Œçš„å•ä¸€å‡½æ•°
```

**å¼‚å¸¸å¤„ç†ä¸å®Œå–„**
```python
# é—®é¢˜ï¼šå¼‚å¸¸å¤„ç†è¿‡äºå®½æ³›
try:
    with open(schema_file, 'r') as f:
        schemas = json.load(f)
except Exception as e:  # å¤ªå®½æ³›
    print(f"Error loading table schemas: {e}")
    sys.exit(1)
```

**ç¼ºä¹è¾“å…¥éªŒè¯**
```python
# é—®é¢˜ï¼šç¼ºä¹å‚æ•°éªŒè¯
bucket_name = os.environ.get('RAW_BUCKET_NAME')
if not bucket_name:  # ä»…æ£€æŸ¥å­˜åœ¨æ€§ï¼Œä¸æ£€æŸ¥æ ¼å¼
    sys.exit(1)
```

#### ğŸŸ¢ ä¼˜åŒ–å»ºè®®

**1. é‡æ„ä¸ºé¢å‘å¯¹è±¡è®¾è®¡**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/glue_manager.py
import boto3
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import logging

class DataCatalogManager(ABC):
    """æ•°æ®ç›®å½•ç®¡ç†æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def create_database(self, database_name: str) -> bool:
        pass
    
    @abstractmethod
    def create_table(self, table_config: Dict) -> bool:
        pass

class GlueTableManager(DataCatalogManager):
    """Glueè¡¨ç®¡ç†å™¨"""
    
    def __init__(self, aws_region: str, validate_permissions: bool = True):
        self.glue_client = boto3.client('glue', region_name=aws_region)
        self.logger = self._setup_logging()
        
        if validate_permissions:
            self._validate_permissions()
    
    def create_database(self, database_name: str) -> bool:
        """åˆ›å»ºGlueæ•°æ®åº“ï¼ŒåŒ…å«å®Œæ•´é”™è¯¯å¤„ç†"""
        try:
            self.glue_client.create_database(
                DatabaseInput={
                    'Name': database_name,
                    'Description': f'Auto-created database for {database_name}'
                }
            )
            self.logger.info(f"Successfully created database: {database_name}")
            return True
        except self.glue_client.exceptions.AlreadyExistsException:
            self.logger.info(f"Database {database_name} already exists")
            return True
        except ClientError as e:
            self.logger.error(f"Failed to create database: {e}")
            return False
    
    def _validate_permissions(self) -> None:
        """éªŒè¯Glueæƒé™"""
        try:
            self.glue_client.get_databases()
        except ClientError as e:
            raise PermissionError(f"Insufficient Glue permissions: {e}")
    
    @staticmethod
    def _setup_logging() -> logging.Logger:
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
```

**2. å®ç°é…ç½®éªŒè¯ç±»**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/config_validator.py
from dataclasses import dataclass
from typing import Optional, List
import re

@dataclass
class ValidationRule:
    """é…ç½®éªŒè¯è§„åˆ™"""
    field_name: str
    required: bool
    pattern: Optional[str] = None
    min_length: Optional[int] = None
    max_length: Optional[int] = None
    allowed_values: Optional[List[str]] = None

class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    AWS_REGION_PATTERN = r'^[a-z]{2}-[a-z]+-[0-9]$'
    BUCKET_NAME_PATTERN = r'^[a-z0-9][a-z0-9\-]*[a-z0-9]$'
    
    def __init__(self):
        self.validation_rules = [
            ValidationRule('AWS_REGION', True, self.AWS_REGION_PATTERN),
            ValidationRule('PROJECT_PREFIX', True, min_length=3, max_length=20),
            ValidationRule('ENVIRONMENT', True, 
                          allowed_values=['dev', 'staging', 'prod'])
        ]
    
    def validate_config(self, config: Dict[str, str]) -> List[str]:
        """éªŒè¯é…ç½®å¹¶è¿”å›é”™è¯¯åˆ—è¡¨"""
        errors = []
        
        for rule in self.validation_rules:
            value = config.get(rule.field_name)
            
            if rule.required and not value:
                errors.append(f"Missing required field: {rule.field_name}")
                continue
            
            if value and rule.pattern:
                if not re.match(rule.pattern, value):
                    errors.append(f"Invalid format for {rule.field_name}: {value}")
            
            if value and rule.allowed_values:
                if value not in rule.allowed_values:
                    errors.append(f"Invalid value for {rule.field_name}: {value}")
        
        return errors
```

**3. å¢å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿—**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/error_handler.py
import logging
import traceback
from functools import wraps
from typing import Callable, Any

class DataLakeError(Exception):
    """æ•°æ®æ¹–æ“ä½œåŸºç¡€å¼‚å¸¸"""
    pass

class ConfigurationError(DataLakeError):
    """é…ç½®ç›¸å…³å¼‚å¸¸"""
    pass

class AWSServiceError(DataLakeError):
    """AWSæœåŠ¡ç›¸å…³å¼‚å¸¸"""
    pass

def handle_aws_errors(func: Callable) -> Callable:
    """AWSæ“ä½œé”™è¯¯å¤„ç†è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        try:
            return func(*args, **kwargs)
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_message = e.response['Error']['Message']
            
            # æ ¹æ®é”™è¯¯ç±»å‹è¿›è¡Œåˆ†ç±»å¤„ç†
            if error_code in ['AccessDenied', 'UnauthorizedOperation']:
                raise AWSServiceError(f"Permission denied: {error_message}")
            elif error_code in ['InvalidParameter', 'ValidationException']:
                raise ConfigurationError(f"Invalid configuration: {error_message}")
            else:
                raise AWSServiceError(f"AWS service error: {error_message}")
        except Exception as e:
            logging.error(f"Unexpected error in {func.__name__}: {traceback.format_exc()}")
            raise DataLakeError(f"Unexpected error: {str(e)}")
    
    return wrapper
```

### 2.3 CloudFormationæ¨¡æ¿è´¨é‡è¯„ä¼°

#### ğŸŸ¢ ä¼˜ç‚¹

**æ¨¡æ¿ç»“æ„æ¸…æ™°**
- å‚æ•°åŒ–è®¾è®¡è‰¯å¥½
- è¾“å‡ºå®šä¹‰å®Œæ•´
- èµ„æºé—´ä¾èµ–å…³ç³»æ˜ç¡®

**å®‰å…¨é…ç½®å®Œå–„**
- S3åŠ å¯†é…ç½®æ­£ç¡®
- è®¿é—®ç­–ç•¥ä¸¥æ ¼
- IAMæƒé™éµå¾ªæœ€å°æƒé™åŸåˆ™

#### ğŸ”´ é—®é¢˜ç‚¹

**ç¼ºä¹æ¨¡æ¿éªŒè¯**
- æ²¡æœ‰è‡ªåŠ¨åŒ–çš„æ¨¡æ¿è¯­æ³•æ£€æŸ¥
- ç¼ºä¹å‚æ•°å€¼èŒƒå›´éªŒè¯
- æ²¡æœ‰èµ„æºå‘½åè§„èŒƒæ£€æŸ¥

#### ğŸŸ¢ ä¼˜åŒ–å»ºè®®

**1. å®ç°æ¨¡æ¿éªŒè¯å·¥å…·**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/validate-templates.sh
#!/bin/bash

validate_cloudformation_templates() {
    print_step "Validating CloudFormation templates..."
    
    local template_dir="templates"
    local validation_errors=0
    
    for template in "$template_dir"/*.yaml; do
        if [[ -f "$template" ]]; then
            print_info "Validating: $(basename "$template")"
            
            # AWS CLIæ¨¡æ¿éªŒè¯
            if ! aws cloudformation validate-template \
                --template-body "file://$template" &>/dev/null; then
                print_error "Template validation failed: $(basename "$template")"
                validation_errors=$((validation_errors + 1))
            fi
            
            # ä½¿ç”¨cfn-lintè¿›è¡Œé«˜çº§éªŒè¯
            if command -v cfn-lint &>/dev/null; then
                if ! cfn-lint "$template" --ignore-checks W3002; then
                    print_warning "CFN-Lint warnings found in: $(basename "$template")"
                fi
            fi
            
            # è‡ªå®šä¹‰éªŒè¯è§„åˆ™
            validate_custom_rules "$template"
        fi
    done
    
    if [[ $validation_errors -eq 0 ]]; then
        print_success "All CloudFormation templates are valid"
    else
        handle_error 1 "Found $validation_errors template validation errors"
    fi
}

validate_custom_rules() {
    local template="$1"
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰S3bucketéƒ½å¯ç”¨äº†åŠ å¯†
    if ! grep -q "BucketEncryption" "$template"; then
        print_warning "S3 bucket encryption not found in: $(basename "$template")"
    fi
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¡¬ç¼–ç çš„è´¦æˆ·IDæˆ–åŒºåŸŸ
    if grep -q "123456789012\|us-east-1" "$template"; then
        print_warning "Possible hardcoded values in: $(basename "$template")"
    fi
}
```

**2. å¢å¼ºæ¨¡æ¿å‚æ•°éªŒè¯**
```yaml
# å»ºè®®æ”¹è¿›ï¼štemplates/s3-storage-layer.yaml
Parameters:
  ProjectPrefix:
    Type: String
    Default: dl-handson
    Description: Prefix for all resource names
    AllowedPattern: ^[a-z0-9][a-z0-9\-]{1,18}[a-z0-9]$
    ConstraintDescription: Must be 3-20 characters, lowercase, alphanumeric with hyphens
    
  Environment:
    Type: String
    Default: dev
    AllowedValues: [dev, staging, prod]
    Description: Environment type
    
  RetentionPolicy:
    Type: String
    Default: Standard
    AllowedValues: [Standard, Extended, Minimal]
    Description: Data retention policy affecting lifecycle rules

Conditions:
  IsProduction: !Equals [!Ref Environment, prod]
  UseExtendedRetention: !Equals [!Ref RetentionPolicy, Extended]

# åŠ¨æ€ç”Ÿå‘½å‘¨æœŸè§„åˆ™é…ç½®
Resources:
  RawDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      LifecycleConfiguration:
        Rules:
          - Id: IntelligentTieringRule
            Status: Enabled
            Transitions:
              - TransitionInDays: !If [IsProduction, 0, 1]
                StorageClass: INTELLIGENT_TIERING
          - Id: ArchiveRule
            Status: !If [UseExtendedRetention, Enabled, Disabled]
            Transitions:
              - TransitionInDays: !If [UseExtendedRetention, 365, 90]
                StorageClass: GLACIER
```

---

## 3. æ¨¡å—åŒ–è®¾è®¡è¯„ä»·

### ğŸ”´ å½“å‰é—®é¢˜

**å•ä½“è„šæœ¬è¿‡å¤§**
- `pyspark_analytics.py` (326è¡Œ) - å•ä¸€å‡½æ•°èŒè´£è¿‡å¤š
- `setup-env.sh` (372è¡Œ) - åŠŸèƒ½è€¦åˆåº¦é«˜
- `deploy-all.sh` (425è¡Œ) - ç¼–æ’é€»è¾‘ä¸ä¸šåŠ¡é€»è¾‘æ··åˆ

**æ¨¡å—é—´ä¾èµ–ä¸æ¸…æ™°**
- ç¼ºä¹æ˜ç¡®çš„APIæ¥å£è®¾è®¡
- é…ç½®ä¼ é€’æ–¹å¼ä¸ç»Ÿä¸€
- é”™è¯¯å¤„ç†æœºåˆ¶ä¸ä¸€è‡´

### ğŸŸ¢ æ¨¡å—åŒ–é‡æ„å»ºè®®

#### 3.1 åˆ›å»ºåˆ†å±‚æ¶æ„

```
scripts/
â”œâ”€â”€ lib/                    # æ ¸å¿ƒåº“
â”‚   â”œâ”€â”€ common/            # é€šç”¨å·¥å…·
â”‚   â”‚   â”œâ”€â”€ logging.sh     # ç»Ÿä¸€æ—¥å¿—
â”‚   â”‚   â”œâ”€â”€ validation.sh  # å‚æ•°éªŒè¯
â”‚   â”‚   â””â”€â”€ retry.sh       # é‡è¯•æœºåˆ¶
â”‚   â”œâ”€â”€ aws/               # AWSæœåŠ¡å°è£…
â”‚   â”‚   â”œâ”€â”€ s3_manager.sh  # S3æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ glue_manager.py # Glueæ“ä½œ
â”‚   â”‚   â””â”€â”€ emr_manager.sh  # EMRæ“ä½œ
â”‚   â””â”€â”€ config/            # é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ env_loader.sh  # ç¯å¢ƒåŠ è½½
â”‚       â””â”€â”€ validator.py   # é…ç½®éªŒè¯
â”œâ”€â”€ core/                  # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ infrastructure/    # åŸºç¡€è®¾æ–½éƒ¨ç½²
â”‚   â”‚   â”œâ”€â”€ s3_deployment.sh
â”‚   â”‚   â”œâ”€â”€ iam_deployment.sh
â”‚   â”‚   â””â”€â”€ lake_formation.sh
â”‚   â”œâ”€â”€ data_processing/   # æ•°æ®å¤„ç†
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”œâ”€â”€ transformation.py
â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â””â”€â”€ deployment/        # éƒ¨ç½²ç¼–æ’
â”‚       â”œâ”€â”€ orchestrator.sh
â”‚       â””â”€â”€ rollback.sh
â””â”€â”€ cli/                   # å‘½ä»¤è¡Œæ¥å£
    â”œâ”€â”€ deploy.sh          # ç®€åŒ–çš„éƒ¨ç½²å…¥å£
    â”œâ”€â”€ cleanup.sh         # ç®€åŒ–çš„æ¸…ç†å…¥å£
    â””â”€â”€ status.sh          # çŠ¶æ€æ£€æŸ¥å…¥å£
```

#### 3.2 å®ç°æ ‡å‡†åŒ–æ¥å£

**1. ç»Ÿä¸€çš„æ¨¡å—æ¥å£**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/lib/interfaces/module_interface.sh
#!/bin/bash

# æ ‡å‡†æ¨¡å—æ¥å£
declare -r MODULE_INTERFACE_VERSION="1.0"

module_interface() {
    local action="$1"
    local module_name="$2"
    shift 2
    
    case "$action" in
        validate)
            "${module_name}_validate" "$@"
            ;;
        deploy)
            "${module_name}_deploy" "$@"
            ;;
        rollback)
            "${module_name}_rollback" "$@"
            ;;
        status)
            "${module_name}_status" "$@"
            ;;
        cleanup)
            "${module_name}_cleanup" "$@"
            ;;
        *)
            print_error "Unknown action: $action"
            return 1
            ;;
    esac
}

# æ¯ä¸ªæ¨¡å—å¿…é¡»å®ç°è¿™äº›å‡½æ•°
require_module_functions() {
    local module_name="$1"
    local required_functions=(
        "${module_name}_validate"
        "${module_name}_deploy"
        "${module_name}_rollback"
        "${module_name}_status"
        "${module_name}_cleanup"
    )
    
    for func in "${required_functions[@]}"; do
        if ! declare -F "$func" >/dev/null; then
            print_error "Module $module_name missing required function: $func"
            return 1
        fi
    done
}
```

**2. é…ç½®ç®¡ç†æ¨¡å—**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/config/config_manager.py
from dataclasses import dataclass, field
from typing import Dict, Any, Optional
from pathlib import Path
import os
import json

@dataclass
class DataLakeConfig:
    """æ•°æ®æ¹–é…ç½®ç±»"""
    
    # åŸºç¡€é…ç½®
    project_prefix: str
    environment: str
    aws_region: str
    
    # S3é…ç½®
    s3_config: Dict[str, Any] = field(default_factory=dict)
    
    # EMRé…ç½®
    emr_config: Dict[str, Any] = field(default_factory=dict)
    
    # å®‰å…¨é…ç½®
    security_config: Dict[str, Any] = field(default_factory=dict)
    
    @classmethod
    def load_from_file(cls, config_path: Path) -> 'DataLakeConfig':
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        if not config_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path) as f:
            config_data = json.load(f)
        
        return cls(**config_data)
    
    @classmethod
    def load_from_env(cls) -> 'DataLakeConfig':
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            project_prefix=os.getenv('PROJECT_PREFIX', 'dl-handson'),
            environment=os.getenv('ENVIRONMENT', 'dev'),
            aws_region=os.getenv('AWS_REGION', 'us-east-1'),
            s3_config={
                'encryption': os.getenv('S3_ENCRYPTION', 'AES256'),
                'versioning': os.getenv('S3_VERSIONING', 'Enabled')
            },
            emr_config={
                'instance_type': os.getenv('EMR_INSTANCE_TYPE', 'm5.xlarge'),
                'instance_count': int(os.getenv('EMR_INSTANCE_COUNT', '3'))
            }
        )
    
    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        errors = []
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if not self.project_prefix:
            errors.append("project_prefix is required")
        
        if self.environment not in ['dev', 'staging', 'prod']:
            errors.append(f"Invalid environment: {self.environment}")
        
        # éªŒè¯EMRé…ç½®
        if self.emr_config.get('instance_count', 0) < 1:
            errors.append("EMR instance_count must be at least 1")
        
        return errors
    
    def get_resource_name(self, resource_type: str, suffix: str = '') -> str:
        """ç”Ÿæˆæ ‡å‡†åŒ–èµ„æºåç§°"""
        parts = [self.project_prefix, resource_type, self.environment]
        if suffix:
            parts.append(suffix)
        return '-'.join(parts)
```

#### 3.3 æ•°æ®å¤„ç†æ¨¡å—é‡æ„

**1. åˆ†ç¦»æ•°æ®å¤„ç†é€»è¾‘**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/core/data_processing/base.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import logging

class DataProcessor(ABC):
    """æ•°æ®å¤„ç†å™¨æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: DataLakeConfig):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    def process(self, input_path: str, output_path: str, **kwargs) -> bool:
        """å¤„ç†æ•°æ®çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    @abstractmethod
    def validate_input(self, input_path: str) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    def setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'/tmp/{self.__class__.__name__}.log'),
                logging.StreamHandler()
            ]
        )

class SparkAnalyticsProcessor(DataProcessor):
    """Sparkåˆ†æå¤„ç†å™¨"""
    
    def __init__(self, config: DataLakeConfig):
        super().__init__(config)
        self.spark_session = self._create_spark_session()
    
    def process(self, input_path: str, output_path: str, **kwargs) -> bool:
        """æ‰§è¡ŒSparkåˆ†æå¤„ç†"""
        try:
            if not self.validate_input(input_path):
                return False
            
            # æ‰§è¡Œå…·ä½“çš„åˆ†æé€»è¾‘
            result = self._run_analytics(input_path, output_path, **kwargs)
            
            self.logger.info(f"Analytics processing completed: {output_path}")
            return result
            
        except Exception as e:
            self.logger.error(f"Analytics processing failed: {e}")
            return False
    
    def validate_input(self, input_path: str) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        # å®ç°Sparkæ•°æ®éªŒè¯é€»è¾‘
        return True
    
    def _create_spark_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„Sparkä¼šè¯"""
        # Sparkä¼šè¯åˆ›å»ºé€»è¾‘
        pass
    
    def _run_analytics(self, input_path: str, output_path: str, **kwargs) -> bool:
        """è¿è¡Œåˆ†æé€»è¾‘"""
        # å®ç°å…·ä½“çš„åˆ†æé€»è¾‘
        pass
```

---

## 4. å®‰å…¨æ€§è¯„ä»·

### ğŸŸ¢ ä¼˜ç‚¹

**IAMæƒé™è®¾è®¡åˆç†**
- è§’è‰²åˆ†ç¦»æ¸…æ™°ï¼ˆAdminã€DataEngineerã€Analystï¼‰
- éµå¾ªæœ€å°æƒé™åŸåˆ™
- Cross-serviceæƒé™é…ç½®æ­£ç¡®

**æ•°æ®åŠ å¯†å®Œå–„**
- S3é™æ€åŠ å¯†é…ç½®
- ä¼ è¾“åŠ å¯†å¼ºåˆ¶æ‰§è¡Œ
- åŠ å¯†å¯†é’¥ç®¡ç†è§„èŒƒ

### ğŸ”´ å®‰å…¨æ¼æ´

**1. å¯†é’¥ç®¡ç†ä¸å½“**
```bash
# é—®é¢˜ï¼šç§é’¥æ–‡ä»¶æƒé™å’Œå­˜å‚¨
KEY_NAME="${prefix}-emr-key-${env}"
aws ec2 create-key-pair --key-name "$KEY_NAME" --query 'KeyMaterial' --output text > "${KEY_NAME}.pem"
chmod 400 "${KEY_NAME}.pem"
```
**é£é™©**: ç§é’¥å­˜å‚¨åœ¨æœ¬åœ°ï¼Œå®¹æ˜“æ³„éœ²

**2. é…ç½®æ–‡ä»¶å®‰å…¨æ€§**
- é…ç½®æ–‡ä»¶ä¸­å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯
- ç¼ºä¹é…ç½®æ–‡ä»¶åŠ å¯†
- ç¯å¢ƒå˜é‡ä¼ é€’ä¸å®‰å…¨

### ğŸŸ¢ å®‰å…¨æ€§æ”¹è¿›å»ºè®®

**1. å®ç°å¯†é’¥ç®¡ç†æœ€ä½³å®è·µ**
```bash
# å»ºè®®æ”¹è¿›ï¼šå®‰å…¨å¯†é’¥ç®¡ç†
secure_key_management() {
    local key_name="$1"
    local vault_path="${HOME}/.datalake/keys"
    
    # åˆ›å»ºå®‰å…¨å¯†é’¥å­˜å‚¨ç›®å½•
    mkdir -p "$vault_path"
    chmod 700 "$vault_path"
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨AWS KMSå¯†é’¥
    local kms_key_id=$(aws kms describe-key --key-id "alias/datalake-${PROJECT_PREFIX}" \
        --query 'KeyMetadata.KeyId' --output text 2>/dev/null)
    
    if [[ -z "$kms_key_id" ]]; then
        print_info "Creating KMS key for secure storage..."
        kms_key_id=$(aws kms create-key --description "DataLake ${PROJECT_PREFIX} encryption key" \
            --query 'KeyMetadata.KeyId' --output text)
        
        aws kms create-alias --alias-name "alias/datalake-${PROJECT_PREFIX}" \
            --target-key-id "$kms_key_id"
    fi
    
    # ä½¿ç”¨AWS SSM Parameter Storeå­˜å‚¨å¯†é’¥
    aws ssm put-parameter \
        --name "/datalake/${PROJECT_PREFIX}/ec2-key" \
        --value "${KEY_NAME}" \
        --type "SecureString" \
        --key-id "$kms_key_id" \
        --overwrite
    
    print_success "Private key securely stored in Parameter Store"
}
```

**2. é…ç½®åŠ å¯†å’ŒéªŒè¯**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/security/config_encryption.py
import boto3
import json
from cryptography.fernet import Fernet
from typing import Dict, Any

class SecureConfigManager:
    """å®‰å…¨é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, kms_key_id: str):
        self.kms_client = boto3.client('kms')
        self.ssm_client = boto3.client('ssm')
        self.kms_key_id = kms_key_id
    
    def encrypt_config(self, config: Dict[str, Any], parameter_name: str) -> bool:
        """åŠ å¯†å¹¶å­˜å‚¨é…ç½®åˆ°SSM Parameter Store"""
        try:
            config_json = json.dumps(config)
            
            # ä½¿ç”¨KMSåŠ å¯†é…ç½®
            response = self.kms_client.encrypt(
                KeyId=self.kms_key_id,
                Plaintext=config_json
            )
            
            # å­˜å‚¨åˆ°SSM Parameter Store
            self.ssm_client.put_parameter(
                Name=parameter_name,
                Value=response['CiphertextBlob'],
                Type='SecureString',
                KeyId=self.kms_key_id,
                Overwrite=True
            )
            
            return True
        except Exception as e:
            print(f"Failed to encrypt config: {e}")
            return False
    
    def decrypt_config(self, parameter_name: str) -> Dict[str, Any]:
        """ä»SSM Parameter Storeè§£å¯†é…ç½®"""
        try:
            # ä»SSMè·å–åŠ å¯†é…ç½®
            response = self.ssm_client.get_parameter(
                Name=parameter_name,
                WithDecryption=True
            )
            
            return json.loads(response['Parameter']['Value'])
        except Exception as e:
            print(f"Failed to decrypt config: {e}")
            return {}
    
    def rotate_encryption_key(self) -> bool:
        """å®šæœŸè½®æ¢åŠ å¯†å¯†é’¥"""
        try:
            self.kms_client.rotate_key_on_demand(KeyId=self.kms_key_id)
            return True
        except Exception as e:
            print(f"Failed to rotate key: {e}")
            return False
```

**3. å®ç°å®‰å…¨å®¡è®¡**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/security/audit.sh
#!/bin/bash

security_audit() {
    print_step "Performing security audit..."
    
    # æ£€æŸ¥IAMæƒé™
    audit_iam_permissions
    
    # æ£€æŸ¥S3æ¡¶å®‰å…¨é…ç½®
    audit_s3_security
    
    # æ£€æŸ¥ç½‘ç»œå®‰å…¨ç»„
    audit_security_groups
    
    # æ£€æŸ¥CloudTrailæ—¥å¿—
    audit_cloudtrail_logs
    
    # ç”Ÿæˆå®‰å…¨æŠ¥å‘Š
    generate_security_report
}

audit_iam_permissions() {
    print_info "Auditing IAM permissions..."
    
    # æ£€æŸ¥è¿‡åº¦æƒé™
    aws iam list-roles --query "Roles[?contains(RoleName, '$PROJECT_PREFIX')]" \
        --output table
    
    # æ£€æŸ¥æœªä½¿ç”¨çš„æƒé™
    for role in $(aws iam list-roles --query "Roles[?contains(RoleName, '$PROJECT_PREFIX')].RoleName" --output text); do
        last_used=$(aws iam get-role --role-name "$role" --query 'Role.RoleLastUsed.LastUsedDate' --output text)
        if [[ "$last_used" == "None" ]]; then
            print_warning "Unused role detected: $role"
        fi
    done
}

audit_s3_security() {
    print_info "Auditing S3 bucket security..."
    
    for bucket in $(aws s3api list-buckets --query 'Buckets[?contains(Name, `'$PROJECT_PREFIX'`)].Name' --output text); do
        # æ£€æŸ¥å…¬å…±è®¿é—®é…ç½®
        public_config=$(aws s3api get-public-access-block --bucket "$bucket" 2>/dev/null)
        if [[ -z "$public_config" ]]; then
            print_warning "No public access block configured for: $bucket"
        fi
        
        # æ£€æŸ¥åŠ å¯†é…ç½®
        encryption=$(aws s3api get-bucket-encryption --bucket "$bucket" 2>/dev/null)
        if [[ -z "$encryption" ]]; then
            print_error "Encryption not configured for: $bucket"
        fi
        
        # æ£€æŸ¥ç‰ˆæœ¬æ§åˆ¶
        versioning=$(aws s3api get-bucket-versioning --bucket "$bucket" --query 'Status' --output text)
        if [[ "$versioning" != "Enabled" ]]; then
            print_warning "Versioning not enabled for: $bucket"
        fi
    done
}
```

---

## 5. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ğŸ”´ å½“å‰æ€§èƒ½é—®é¢˜

**1. è„šæœ¬æ‰§è¡Œæ•ˆç‡ä½**
- é¡ºåºæ‰§è¡Œå¯¼è‡´éƒ¨ç½²æ—¶é—´é•¿
- ç¼ºä¹å¹¶è¡ŒåŒ–å¤„ç†
- é‡å¤çš„AWS APIè°ƒç”¨

**2. èµ„æºåˆ©ç”¨ä¸å……åˆ†**
- EMRé›†ç¾¤å›ºå®šå¤§å°
- ç¼ºä¹è‡ªåŠ¨æ‰©ç¼©å®¹
- Sparké…ç½®æœªä¼˜åŒ–

### ğŸŸ¢ æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

**1. å¹¶è¡ŒåŒ–éƒ¨ç½²**
```bash
# å»ºè®®æ”¹è¿›ï¼šå¹¶è¡Œéƒ¨ç½²æ¶æ„
parallel_deployment() {
    print_step "Starting parallel deployment..."
    
    # å®šä¹‰éƒ¨ç½²ä»»åŠ¡ç»„
    declare -A deployment_groups=(
        ["infrastructure"]="s3_deployment iam_deployment"
        ["catalog"]="glue_deployment lake_formation_deployment"  
        ["compute"]="emr_deployment"
    )
    
    # å¹¶è¡Œæ‰§è¡ŒåŒç»„ä»»åŠ¡
    for group in "${!deployment_groups[@]}"; do
        print_info "Deploying group: $group"
        
        local tasks=(${deployment_groups[$group]})
        local pids=()
        
        # å¹¶è¡Œå¯åŠ¨ä»»åŠ¡
        for task in "${tasks[@]}"; do
            $task &
            pids+=($!)
        done
        
        # ç­‰å¾…ç»„å†…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        local failed_tasks=0
        for i in "${!pids[@]}"; do
            if ! wait "${pids[$i]}"; then
                print_error "Task ${tasks[$i]} failed"
                failed_tasks=$((failed_tasks + 1))
            fi
        done
        
        if [[ $failed_tasks -gt 0 ]]; then
            print_error "Group $group failed with $failed_tasks failed tasks"
            return 1
        fi
        
        print_success "Group $group completed successfully"
    done
}
```

**2. æ™ºèƒ½èµ„æºç®¡ç†**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/core/resource_optimizer.py
import boto3
from typing import Dict, List
import json

class ResourceOptimizer:
    """èµ„æºä¼˜åŒ–å™¨"""
    
    def __init__(self, config: DataLakeConfig):
        self.config = config
        self.emr_client = boto3.client('emr')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def optimize_emr_cluster(self, cluster_id: str) -> Dict[str, Any]:
        """åŸºäºå†å²ä½¿ç”¨æƒ…å†µä¼˜åŒ–EMRé›†ç¾¤"""
        
        # è·å–é›†ç¾¤ä½¿ç”¨æŒ‡æ ‡
        metrics = self._get_cluster_metrics(cluster_id)
        
        # è®¡ç®—æœ€ä¼˜é…ç½®
        optimal_config = self._calculate_optimal_config(metrics)
        
        # åº”ç”¨é…ç½®å»ºè®®
        recommendations = {
            'instance_type': optimal_config['instance_type'],
            'instance_count': optimal_config['instance_count'],
            'spot_instances': optimal_config['use_spot'],
            'auto_scaling': optimal_config['auto_scaling_config']
        }
        
        return recommendations
    
    def _get_cluster_metrics(self, cluster_id: str) -> Dict:
        """è·å–é›†ç¾¤æ€§èƒ½æŒ‡æ ‡"""
        
        metrics = {}
        
        # CPUåˆ©ç”¨ç‡
        cpu_response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ElasticMapReduce',
            MetricName='CPUUtilization',
            Dimensions=[{'Name': 'JobFlowId', 'Value': cluster_id}],
            StartTime=datetime.utcnow() - timedelta(days=7),
            EndTime=datetime.utcnow(),
            Period=3600,
            Statistics=['Average', 'Maximum']
        )
        
        metrics['cpu'] = {
            'avg': sum(p['Average'] for p in cpu_response['Datapoints']) / len(cpu_response['Datapoints']),
            'max': max(p['Maximum'] for p in cpu_response['Datapoints'])
        }
        
        # å†…å­˜åˆ©ç”¨ç‡
        memory_response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ElasticMapReduce',
            MetricName='MemoryPercentage',
            Dimensions=[{'Name': 'JobFlowId', 'Value': cluster_id}],
            StartTime=datetime.utcnow() - timedelta(days=7),
            EndTime=datetime.utcnow(),
            Period=3600,
            Statistics=['Average', 'Maximum']
        )
        
        metrics['memory'] = {
            'avg': sum(p['Average'] for p in memory_response['Datapoints']) / len(memory_response['Datapoints']),
            'max': max(p['Maximum'] for p in memory_response['Datapoints'])
        }
        
        return metrics
    
    def _calculate_optimal_config(self, metrics: Dict) -> Dict:
        """åŸºäºæŒ‡æ ‡è®¡ç®—æœ€ä¼˜é…ç½®"""
        
        config = {}
        
        # åŸºäºCPUä½¿ç”¨ç‡è°ƒæ•´å®ä¾‹ç±»å‹
        if metrics['cpu']['avg'] > 80:
            config['instance_type'] = 'm5.2xlarge'  # å‡çº§
        elif metrics['cpu']['avg'] < 30:
            config['instance_type'] = 'm5.large'    # é™çº§
        else:
            config['instance_type'] = 'm5.xlarge'   # ä¿æŒ
        
        # åŸºäºè´Ÿè½½å˜åŒ–é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹
        if metrics['cpu']['max'] - metrics['cpu']['avg'] > 30:
            config['auto_scaling_config'] = {
                'min_capacity': 1,
                'max_capacity': 10,
                'target_on_demand_capacity': 2,
                'target_spot_capacity': 8
            }
            config['use_spot'] = True
        else:
            config['auto_scaling_config'] = None
            config['use_spot'] = False
        
        # è®¡ç®—æœ€ä¼˜å®ä¾‹æ•°é‡
        config['instance_count'] = max(1, int(metrics['cpu']['avg'] / 20))
        
        return config
```

**3. ç¼“å­˜å’Œæ‰¹é‡æ“ä½œä¼˜åŒ–**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/lib/aws/batch_operations.sh
#!/bin/bash

# AWSæ‰¹é‡æ“ä½œä¼˜åŒ–
batch_s3_operations() {
    local operation="$1"
    local bucket="$2"
    shift 2
    local files=("$@")
    
    case "$operation" in
        upload)
            batch_s3_upload "$bucket" "${files[@]}"
            ;;
        download)
            batch_s3_download "$bucket" "${files[@]}"
            ;;
        delete)
            batch_s3_delete "$bucket" "${files[@]}"
            ;;
        *)
            print_error "Unknown batch operation: $operation"
            return 1
            ;;
    esac
}

batch_s3_upload() {
    local bucket="$1"
    shift
    local files=("$@")
    
    print_info "Starting batch upload to s3://$bucket"
    
    # ä½¿ç”¨AWS CLIçš„åŒæ­¥åŠŸèƒ½ï¼Œæ”¯æŒå¹¶å‘ä¸Šä¼ 
    local temp_manifest="/tmp/upload_manifest_$$"
    printf "%s\n" "${files[@]}" > "$temp_manifest"
    
    # ä½¿ç”¨xargså®ç°å¹¶å‘ä¸Šä¼ 
    cat "$temp_manifest" | xargs -I {} -P 8 aws s3 cp {} "s3://$bucket/"
    
    local exit_code=$?
    rm -f "$temp_manifest"
    
    if [[ $exit_code -eq 0 ]]; then
        print_success "Batch upload completed"
    else
        print_error "Batch upload failed"
    fi
    
    return $exit_code
}

# APIè°ƒç”¨ç¼“å­˜
declare -A aws_call_cache

cached_aws_call() {
    local cache_key="$1"
    local cache_ttl="${2:-300}"  # é»˜è®¤5åˆ†é’Ÿç¼“å­˜
    shift 2
    
    local cache_file="/tmp/aws_cache_${cache_key//[\/:]/_}"
    local current_time=$(date +%s)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if [[ -f "$cache_file" ]]; then
        local cache_time=$(stat -f %m "$cache_file" 2>/dev/null || stat -c %Y "$cache_file")
        if [[ $((current_time - cache_time)) -lt $cache_ttl ]]; then
            cat "$cache_file"
            return 0
        fi
    fi
    
    # æ‰§è¡ŒAWSè°ƒç”¨å¹¶ç¼“å­˜ç»“æœ
    if "$@" > "$cache_file"; then
        cat "$cache_file"
        return 0
    else
        rm -f "$cache_file"
        return 1
    fi
}

# ç¤ºä¾‹ä½¿ç”¨
get_bucket_policy() {
    local bucket="$1"
    cached_aws_call "bucket_policy_$bucket" 600 \
        aws s3api get-bucket-policy --bucket "$bucket" --query 'Policy' --output text
}
```

---

## 6. ç›‘æ§å’Œå¯è§‚å¯Ÿæ€§å»ºè®®

### ğŸ”´ å½“å‰ç›‘æ§ç¼ºå¤±

**ç¼ºä¹è¿è¡Œæ—¶ç›‘æ§**
- æ²¡æœ‰åº”ç”¨çº§åˆ«çš„æŒ‡æ ‡æ”¶é›†
- ç¼ºä¹æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
- é”™è¯¯å‘Šè­¦æœºåˆ¶ä¸å®Œå–„

**æ—¥å¿—ç®¡ç†ä¸ç»Ÿä¸€**
- æ—¥å¿—æ ¼å¼ä¸ä¸€è‡´
- ç¼ºä¹é›†ä¸­åŒ–æ—¥å¿—æ”¶é›†
- è°ƒè¯•ä¿¡æ¯ä¸å……åˆ†

### ğŸŸ¢ ç›‘æ§ä½“ç³»å»ºè®®

**1. å®ç°åˆ†å¸ƒå¼è¿½è¸ª**
```python
# å»ºè®®åˆ›å»ºï¼šscripts/lib/monitoring/tracer.py
import logging
import time
import json
import uuid
from typing import Dict, Any, Optional
from contextlib import contextmanager

class DataLakeTracer:
    """æ•°æ®æ¹–æ“ä½œè¿½è¸ªå™¨"""
    
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.trace_id = str(uuid.uuid4())
        self.spans: List[Dict] = []
        self.logger = self._setup_logger()
    
    @contextmanager
    def span(self, operation_name: str, **metadata):
        """åˆ›å»ºè¿½è¸ªspan"""
        span_id = str(uuid.uuid4())
        start_time = time.time()
        
        span_data = {
            'trace_id': self.trace_id,
            'span_id': span_id,
            'operation_name': operation_name,
            'service_name': self.service_name,
            'start_time': start_time,
            'metadata': metadata
        }
        
        try:
            self.logger.info(f"Starting operation: {operation_name}", extra=span_data)
            yield span_data
            
            # æ“ä½œæˆåŠŸ
            span_data['status'] = 'success'
            
        except Exception as e:
            # æ“ä½œå¤±è´¥
            span_data['status'] = 'error'
            span_data['error'] = str(e)
            self.logger.error(f"Operation failed: {operation_name}", extra=span_data)
            raise
            
        finally:
            span_data['end_time'] = time.time()
            span_data['duration'] = span_data['end_time'] - span_data['start_time']
            
            self.spans.append(span_data)
            self._send_to_monitoring(span_data)
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—"""
        logger = logging.getLogger(f"datalake.{self.service_name}")
        
        # è‡ªå®šä¹‰æ ¼å¼å™¨ï¼Œè¾“å‡ºJSONæ ¼å¼æ—¥å¿—
        formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "level": "%(levelname)s", '
            '"service": "%(name)s", "message": "%(message)s", '
            '"trace_id": "%(trace_id)s", "span_id": "%(span_id)s"}'
        )
        
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        
        return logger
    
    def _send_to_monitoring(self, span_data: Dict):
        """å‘é€è¿½è¸ªæ•°æ®åˆ°ç›‘æ§ç³»ç»Ÿ"""
        try:
            # å‘é€åˆ°CloudWatchè‡ªå®šä¹‰æŒ‡æ ‡
            cloudwatch = boto3.client('cloudwatch')
            
            cloudwatch.put_metric_data(
                Namespace='DataLake/Operations',
                MetricData=[
                    {
                        'MetricName': 'OperationDuration',
                        'Value': span_data['duration'],
                        'Unit': 'Seconds',
                        'Dimensions': [
                            {'Name': 'Service', 'Value': span_data['service_name']},
                            {'Name': 'Operation', 'Value': span_data['operation_name']},
                            {'Name': 'Status', 'Value': span_data['status']}
                        ]
                    }
                ]
            )
            
        except Exception as e:
            self.logger.warning(f"Failed to send monitoring data: {e}")

# ä½¿ç”¨ç¤ºä¾‹
def deploy_with_tracing():
    tracer = DataLakeTracer("deployment_service")
    
    with tracer.span("s3_bucket_creation", bucket_count=4):
        # S3æ¡¶åˆ›å»ºé€»è¾‘
        time.sleep(1)  # æ¨¡æ‹Ÿæ“ä½œ
    
    with tracer.span("iam_role_creation", role_count=7):
        # IAMè§’è‰²åˆ›å»ºé€»è¾‘
        time.sleep(2)  # æ¨¡æ‹Ÿæ“ä½œ
```

**2. å¥åº·æ£€æŸ¥å’Œå‘Šè­¦**
```bash
# å»ºè®®åˆ›å»ºï¼šscripts/monitoring/health_check.sh
#!/bin/bash

health_check() {
    local health_status="healthy"
    local issues=()
    
    print_step "Performing health check..."
    
    # æ£€æŸ¥CloudFormationå †æ ˆçŠ¶æ€
    check_cloudformation_health
    if [[ $? -ne 0 ]]; then
        health_status="unhealthy"
        issues+=("CloudFormation stacks have issues")
    fi
    
    # æ£€æŸ¥S3æ¡¶å¯è®¿é—®æ€§
    check_s3_health
    if [[ $? -ne 0 ]]; then
        health_status="unhealthy"
        issues+=("S3 buckets are not accessible")
    fi
    
    # æ£€æŸ¥EMRé›†ç¾¤çŠ¶æ€
    check_emr_health
    if [[ $? -ne 0 ]]; then
        health_status="degraded"
        issues+=("EMR cluster performance issues")
    fi
    
    # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
    check_data_freshness
    if [[ $? -ne 0 ]]; then
        health_status="degraded" 
        issues+=("Data is not fresh")
    fi
    
    # ç”Ÿæˆå¥åº·æŠ¥å‘Š
    generate_health_report "$health_status" "${issues[@]}"
    
    # å‘é€å‘Šè­¦
    if [[ "$health_status" != "healthy" ]]; then
        send_alert "$health_status" "${issues[@]}"
    fi
}

check_cloudformation_health() {
    local failed_stacks=0
    
    for stack in $(list_project_stacks); do
        local status=$(aws cloudformation describe-stacks \
            --stack-name "$stack" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null)
        
        case "$status" in
            CREATE_COMPLETE|UPDATE_COMPLETE)
                print_success "Stack healthy: $stack"
                ;;
            *FAILED*|*ROLLBACK*)
                print_error "Stack unhealthy: $stack ($status)"
                failed_stacks=$((failed_stacks + 1))
                ;;
            *IN_PROGRESS*)
                print_warning "Stack in progress: $stack ($status)"
                ;;
        esac
    done
    
    return $failed_stacks
}

check_data_freshness() {
    local max_age_hours=24
    local current_time=$(date +%s)
    
    # æ£€æŸ¥å„å±‚æ•°æ®çš„æœ€æ–°æ›´æ–°æ—¶é—´
    for layer in raw clean analytics; do
        local bucket="${PROJECT_PREFIX}-${layer}-${ENVIRONMENT}"
        local last_modified=$(aws s3api list-objects-v2 \
            --bucket "$bucket" \
            --query 'sort_by(Contents, &LastModified)[-1].LastModified' \
            --output text 2>/dev/null)
        
        if [[ -n "$last_modified" ]]; then
            local mod_time=$(date -d "$last_modified" +%s)
            local age_hours=$(( (current_time - mod_time) / 3600 ))
            
            if [[ $age_hours -gt $max_age_hours ]]; then
                print_warning "Stale data in $layer layer: ${age_hours}h old"
                return 1
            else
                print_success "Fresh data in $layer layer: ${age_hours}h old"
            fi
        else
            print_warning "No data found in $layer layer"
            return 1
        fi
    done
    
    return 0
}

send_alert() {
    local status="$1"
    shift
    local issues=("$@")
    
    # å‘é€SNSå‘Šè­¦
    local topic_arn="arn:aws:sns:${AWS_REGION}:${AWS_ACCOUNT_ID}:datalake-alerts"
    
    local message=$(cat <<EOF
{
    "timestamp": "$(date -Iseconds)",
    "project": "${PROJECT_PREFIX}",
    "environment": "${ENVIRONMENT}",
    "status": "${status}",
    "issues": $(printf '%s\n' "${issues[@]}" | jq -R . | jq -s .)
}
EOF
    )
    
    aws sns publish \
        --topic-arn "$topic_arn" \
        --subject "DataLake Health Alert: $status" \
        --message "$message"
}

# å®šæœŸå¥åº·æ£€æŸ¥ä»»åŠ¡
setup_health_monitoring() {
    print_step "Setting up health monitoring..."
    
    # åˆ›å»ºcronä»»åŠ¡è¿›è¡Œå®šæœŸæ£€æŸ¥
    local cron_job="*/15 * * * * $PWD/scripts/monitoring/health_check.sh"
    
    # æ·»åŠ åˆ°crontab
    (crontab -l 2>/dev/null; echo "$cron_job") | crontab -
    
    print_success "Health monitoring cron job installed"
}
```

---

## 7. ç»¼åˆä¼˜åŒ–å»ºè®®æ€»ç»“

### ğŸš€ é«˜ä¼˜å…ˆçº§æ”¹è¿› (1-2ä¸ªæœˆ)

1. **æ¨¡å—åŒ–é‡æ„**
   - æ‹†åˆ†å•ä½“è„šæœ¬ä¸ºåŠŸèƒ½æ¨¡å—
   - å®ç°æ ‡å‡†åŒ–æ¨¡å—æ¥å£
   - åˆ›å»ºé€šç”¨å‡½æ•°åº“

2. **é”™è¯¯å¤„ç†å¢å¼º**
   - å®ç°ç»Ÿä¸€é”™è¯¯å¤„ç†æœºåˆ¶
   - æ·»åŠ é‡è¯•å’Œæ¢å¤é€»è¾‘
   - å®Œå–„æ—¥å¿—è®°å½•

3. **å®‰å…¨æ€§åŠ å›º**
   - å®ç°å¯†é’¥ç®¡ç†æœ€ä½³å®è·µ
   - é…ç½®æ–‡ä»¶åŠ å¯†å­˜å‚¨
   - æ·»åŠ å®‰å…¨å®¡è®¡è„šæœ¬

### ğŸ¯ ä¸­ä¼˜å…ˆçº§æ”¹è¿› (2-4ä¸ªæœˆ)

4. **æ€§èƒ½ä¼˜åŒ–**
   - å¹¶è¡ŒåŒ–éƒ¨ç½²æµç¨‹
   - å®ç°æ™ºèƒ½èµ„æºç®¡ç†
   - ä¼˜åŒ–AWS APIè°ƒç”¨

5. **ç›‘æ§ä½“ç³»**
   - å®ç°åˆ†å¸ƒå¼è¿½è¸ª
   - æ·»åŠ å¥åº·æ£€æŸ¥
   - å»ºç«‹å‘Šè­¦æœºåˆ¶

6. **æµ‹è¯•è¦†ç›–**
   - å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
   - è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿
   - æ€§èƒ½æµ‹è¯•

### ğŸ“ˆ ä½ä¼˜å…ˆçº§æ”¹è¿› (4-6ä¸ªæœˆ)

7. **é«˜çº§åŠŸèƒ½**
   - å¤šåŒºåŸŸéƒ¨ç½²æ”¯æŒ
   - è“ç»¿éƒ¨ç½²ç­–ç•¥
   - è‡ªåŠ¨å›æ»šæœºåˆ¶

8. **å¯è§†åŒ–å’ŒUI**
   - Webç•Œé¢ç®¡ç†
   - å¯è§†åŒ–ç›‘æ§ä»ªè¡¨æ¿
   - æ“ä½œå®¡è®¡ç•Œé¢

### ğŸ’¡ æ¶æ„æ¼”è¿›è·¯å¾„

```mermaid
graph TD
    A[å½“å‰çŠ¶æ€: å•ä½“è„šæœ¬] --> B[ç¬¬ä¸€é˜¶æ®µ: æ¨¡å—åŒ–é‡æ„]
    B --> C[ç¬¬äºŒé˜¶æ®µ: æœåŠ¡åŒ–]
    C --> D[ç¬¬ä¸‰é˜¶æ®µ: å¾®æœåŠ¡æ¶æ„]
    D --> E[ç¬¬å››é˜¶æ®µ: äº‘åŸç”ŸåŒ–]
    
    B1[ç»Ÿä¸€æ¥å£] --> B
    B2[é”™è¯¯å¤„ç†] --> B
    B3[å®‰å…¨åŠ å›º] --> B
    
    C1[API Gateway] --> C
    C2[æœåŠ¡å‘ç°] --> C
    C3[é…ç½®ä¸­å¿ƒ] --> C
    
    D1[å®¹å™¨åŒ–] --> D
    D2[æœåŠ¡ç½‘æ ¼] --> D
    D3[è‡ªåŠ¨æ‰©ç¼©å®¹] --> D
```

---

## 8. å®æ–½å»ºè®®

### ğŸ“‹ å®æ–½è®¡åˆ’

**Phase 1: åŸºç¡€é‡æ„ (4å‘¨)**
- Week 1-2: æ¨¡å—åŒ–é‡æ„å’Œé€šç”¨åº“åˆ›å»º
- Week 3: é”™è¯¯å¤„ç†å’Œæ—¥å¿—æ ‡å‡†åŒ–  
- Week 4: å®‰å…¨æ€§æ”¹è¿›å’Œæµ‹è¯•

**Phase 2: æ€§èƒ½å’Œç›‘æ§ (4å‘¨)**
- Week 5-6: å¹¶è¡ŒåŒ–å’Œæ€§èƒ½ä¼˜åŒ–
- Week 7: ç›‘æ§ä½“ç³»å®æ–½
- Week 8: é›†æˆæµ‹è¯•å’Œæ–‡æ¡£æ›´æ–°

**Phase 3: é«˜çº§ç‰¹æ€§ (4å‘¨)**
- Week 9-10: é«˜çº§åŠŸèƒ½å¼€å‘
- Week 11: å¯è§†åŒ–ç•Œé¢
- Week 12: å®Œæ•´æ€§æµ‹è¯•å’Œä¸Šçº¿

### ğŸ¯ æˆåŠŸæŒ‡æ ‡

**æŠ€æœ¯æŒ‡æ ‡**
- éƒ¨ç½²æ—¶é—´å‡å°‘60%
- æ•…éšœæ¢å¤æ—¶é—´å‡å°‘80%
- ä»£ç é‡å¤ç‡é™ä½è‡³5%ä»¥ä¸‹
- æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°85%ä»¥ä¸Š

**ä¸šåŠ¡æŒ‡æ ‡**
- ç³»ç»Ÿå¯ç”¨æ€§æå‡è‡³99.9%
- å¼€å‘æ•ˆç‡æå‡50%
- è¿ç»´æˆæœ¬é™ä½30%
- å®‰å…¨æ¼æ´å‡å°‘90%

---

## æ€»ç»“

è¿™ä¸ªAWSæ•°æ®æ¹–é¡¹ç›®è™½ç„¶åœ¨æ¶æ„è®¾è®¡å’ŒåŠŸèƒ½å®ç°ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä»£ç è´¨é‡ã€æ¨¡å—åŒ–è®¾è®¡å’Œå¯ç»´æŠ¤æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„é‡æ„å’Œä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æå‡é¡¹ç›®çš„å·¥ç¨‹è´¨é‡å’Œé•¿æœŸå¯ç»´æŠ¤æ€§ã€‚

å»ºè®®ä¼˜å…ˆå®æ–½æ¨¡å—åŒ–é‡æ„ã€é”™è¯¯å¤„ç†å¢å¼ºå’Œå®‰å…¨æ€§åŠ å›ºï¼Œè¿™äº›æ”¹è¿›å°†ä¸ºåç»­çš„æ€§èƒ½ä¼˜åŒ–å’Œé«˜çº§åŠŸèƒ½æä¾›åšå®çš„åŸºç¡€ã€‚

**æœ€ç»ˆç›®æ ‡**: æ„å»ºä¸€ä¸ªé«˜è´¨é‡ã€å¯æ‰©å±•ã€å®‰å…¨å¯é çš„ä¼ä¸šçº§æ•°æ®æ¹–å¹³å°ï¼Œä¸ºç»„ç»‡çš„æ•°æ®é©±åŠ¨å†³ç­–æä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚