#!/usr/bin/env python3
"""
ç”µå•†æ•°æ®åˆ†æå¤„ç†å™¨
ä¸“é—¨ç”¨äºå¤„ç†ç”µå•†æ•°æ®çš„åˆ†æä»»åŠ¡ï¼Œç»§æ‰¿è‡ªåŸºç¡€SparkAnalyticsProcessor
"""

import os
import sys
from typing import Dict, Any, Optional
from datetime import datetime

# æ·»åŠ libç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'lib'))

from data_processing import SparkAnalyticsProcessor, DataLakeConfig, ProcessingResult

try:
    from pyspark.sql import DataFrame
    from pyspark.sql.functions import (
        col, sum as spark_sum, avg, count, max as spark_max, min as spark_min,
        year, month, dayofmonth, date_format, when, isnan, isnull,
        regexp_replace, trim, upper, lower, concat_ws, round as spark_round,
        desc, asc, coalesce, lit
    )
    from pyspark.sql.types import *
except ImportError:
    print("è­¦å‘Š: PySparkæœªå®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å°†ä¸å¯ç”¨")


class EcommerceAnalyticsProcessor(SparkAnalyticsProcessor):
    """ç”µå•†æ•°æ®åˆ†æå¤„ç†å™¨"""
    
    def __init__(self, config: DataLakeConfig):
        super().__init__(config)
        
        # ç”µå•†ç‰¹å®šçš„é…ç½®
        self.clean_bucket = config.clean_bucket
        self.analytics_bucket = config.analytics_bucket
    
    def read_ecommerce_data(self) -> Dict[str, DataFrame]:
        """è¯»å–ç”µå•†æ•°æ®çš„æ‰€æœ‰è¡¨"""
        self.logger.info(f"ä»æ¸…æ´å±‚è¯»å–ç”µå•†æ•°æ®: {self.clean_bucket}")
        
        data = {}
        tables = ['customers', 'products', 'orders', 'order_items']
        
        for table in tables:
            try:
                path = f"s3a://{self.clean_bucket}/ecommerce/{table}/"
                df = self.read_data(path, "csv")
                
                # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
                df = self._clean_dataframe(df, table)
                data[table] = df
                
                self.logger.info(f"æˆåŠŸè¯»å– {table}: {df.count()} è¡Œ")
                
            except Exception as e:
                self.logger.error(f"è¯»å– {table} å¤±è´¥: {str(e)}")
                raise
        
        return data
    
    def _clean_dataframe(self, df: DataFrame, table_name: str) -> DataFrame:
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®æ¡†"""
        self.logger.debug(f"æ¸…æ´—æ•°æ®æ¡†: {table_name}")
        
        # é€šç”¨æ¸…æ´—ï¼šå»é™¤ç©ºå€¼å’Œé‡å¤é¡¹
        df = df.dropDuplicates()
        
        # è¡¨ç‰¹å®šçš„æ¸…æ´—
        if table_name == 'customers':
            df = self._clean_customers(df)
        elif table_name == 'products':
            df = self._clean_products(df)
        elif table_name == 'orders':
            df = self._clean_orders(df)
        elif table_name == 'order_items':
            df = self._clean_order_items(df)
        
        return df
    
    def _clean_customers(self, df: DataFrame) -> DataFrame:
        """æ¸…æ´—å®¢æˆ·æ•°æ®"""
        return df.select(
            col("customer_id").cast(IntegerType()),
            trim(upper(col("customer_name"))).alias("customer_name"),
            trim(lower(col("email"))).alias("email"),
            trim(col("phone")).alias("phone"),
            trim(col("address")).alias("address"),
            trim(col("city")).alias("city"),
            trim(col("state")).alias("state"),
            trim(col("country")).alias("country"),
            col("registration_date").cast(DateType()),
            trim(col("customer_segment")).alias("customer_segment")
        ).filter(col("customer_id").isNotNull())
    
    def _clean_products(self, df: DataFrame) -> DataFrame:
        """æ¸…æ´—äº§å“æ•°æ®"""
        return df.select(
            col("product_id").cast(IntegerType()),
            trim(col("product_name")).alias("product_name"),
            trim(col("category")).alias("category"),
            trim(col("subcategory")).alias("subcategory"),
            col("price").cast(DecimalType(10, 2)),
            col("cost").cast(DecimalType(10, 2)),
            trim(col("supplier")).alias("supplier"),
            col("launch_date").cast(DateType())
        ).filter(col("product_id").isNotNull() & (col("price") > 0))
    
    def _clean_orders(self, df: DataFrame) -> DataFrame:
        """æ¸…æ´—è®¢å•æ•°æ®"""
        return df.select(
            col("order_id").cast(IntegerType()),
            col("customer_id").cast(IntegerType()),
            col("order_date").cast(DateType()),
            col("ship_date").cast(DateType()),
            trim(col("ship_mode")).alias("ship_mode"),
            trim(col("segment")).alias("segment"),
            trim(col("region")).alias("region"),
            col("total_amount").cast(DecimalType(12, 2)),
            trim(col("order_status")).alias("order_status")
        ).filter(
            col("order_id").isNotNull() & 
            col("customer_id").isNotNull() & 
            (col("total_amount") > 0)
        )
    
    def _clean_order_items(self, df: DataFrame) -> DataFrame:
        """æ¸…æ´—è®¢å•é¡¹æ•°æ®"""
        return df.select(
            col("order_id").cast(IntegerType()),
            col("product_id").cast(IntegerType()),
            col("quantity").cast(IntegerType()),
            col("unit_price").cast(DecimalType(10, 2)),
            col("discount").cast(DecimalType(5, 4)),
            (col("quantity") * col("unit_price") * (1 - col("discount"))).alias("line_total")
        ).filter(
            col("order_id").isNotNull() & 
            col("product_id").isNotNull() & 
            (col("quantity") > 0) & 
            (col("unit_price") > 0)
        )
    
    def create_customer_analytics(self, data: Dict[str, DataFrame]) -> DataFrame:
        """åˆ›å»ºå®¢æˆ·åˆ†ææ•°æ®"""
        self.logger.info("åˆ›å»ºå®¢æˆ·åˆ†ææ•°æ®")
        
        customers = data['customers']
        orders = data['orders']
        order_items = data['order_items']
        
        # å®¢æˆ·è®¢å•ç»Ÿè®¡
        customer_order_stats = orders.groupBy("customer_id").agg(
            count("order_id").alias("total_orders"),
            spark_sum("total_amount").alias("total_revenue"),
            avg("total_amount").alias("avg_order_value"),
            spark_max("order_date").alias("last_order_date"),
            spark_min("order_date").alias("first_order_date")
        )
        
        # å®¢æˆ·äº§å“åå¥½
        customer_product_stats = order_items.join(orders, "order_id") \
            .groupBy("customer_id") \
            .agg(
                spark_sum("quantity").alias("total_items_purchased"),
                count("product_id").alias("unique_products_purchased")
            )
        
        # åˆå¹¶å®¢æˆ·åˆ†ææ•°æ®
        customer_analytics = customers.join(customer_order_stats, "customer_id", "left") \
            .join(customer_product_stats, "customer_id", "left") \
            .select(
                col("customer_id"),
                col("customer_name"),
                col("email"),
                col("city"),
                col("state"),
                col("country"),
                col("customer_segment"),
                col("registration_date"),
                coalesce(col("total_orders"), lit(0)).alias("total_orders"),
                coalesce(col("total_revenue"), lit(0)).alias("total_revenue"),
                coalesce(col("avg_order_value"), lit(0)).alias("avg_order_value"),
                coalesce(col("total_items_purchased"), lit(0)).alias("total_items_purchased"),
                coalesce(col("unique_products_purchased"), lit(0)).alias("unique_products_purchased"),
                col("last_order_date"),
                col("first_order_date")
            )
        
        return customer_analytics
    
    def create_product_analytics(self, data: Dict[str, DataFrame]) -> DataFrame:
        """åˆ›å»ºäº§å“åˆ†ææ•°æ®"""
        self.logger.info("åˆ›å»ºäº§å“åˆ†ææ•°æ®")
        
        products = data['products']
        orders = data['orders']
        order_items = data['order_items']
        
        # äº§å“é”€å”®ç»Ÿè®¡
        product_sales = order_items.join(orders, "order_id") \
            .groupBy("product_id") \
            .agg(
                spark_sum("quantity").alias("total_quantity_sold"),
                count("order_id").alias("total_orders"),
                spark_sum("line_total").alias("total_revenue"),
                avg("unit_price").alias("avg_selling_price"),
                countDistinct("customer_id").alias("unique_customers")
            )
        
        # åˆå¹¶äº§å“åˆ†ææ•°æ®
        product_analytics = products.join(product_sales, "product_id", "left") \
            .select(
                col("product_id"),
                col("product_name"),
                col("category"),
                col("subcategory"),
                col("price"),
                col("cost"),
                (col("price") - col("cost")).alias("margin"),
                col("supplier"),
                col("launch_date"),
                coalesce(col("total_quantity_sold"), lit(0)).alias("total_quantity_sold"),
                coalesce(col("total_orders"), lit(0)).alias("total_orders"),
                coalesce(col("total_revenue"), lit(0)).alias("total_revenue"),
                coalesce(col("avg_selling_price"), lit(0)).alias("avg_selling_price"),
                coalesce(col("unique_customers"), lit(0)).alias("unique_customers")
            )
        
        return product_analytics
    
    def create_monthly_sales_analytics(self, data: Dict[str, DataFrame]) -> DataFrame:
        """åˆ›å»ºæœˆåº¦é”€å”®åˆ†ææ•°æ®"""
        self.logger.info("åˆ›å»ºæœˆåº¦é”€å”®åˆ†ææ•°æ®")
        
        orders = data['orders']
        order_items = data['order_items']
        products = data['products']
        
        # åˆå¹¶è®¢å•å’Œè®¢å•é¡¹æ•°æ®
        sales_data = orders.join(order_items, "order_id") \
            .join(products, "product_id")
        
        # æŒ‰å¹´æœˆåˆ†ç»„åˆ†æ
        monthly_sales = sales_data.select(
            year("order_date").alias("year"),
            month("order_date").alias("month"),
            date_format("order_date", "yyyy-MM").alias("year_month"),
            col("category"),
            col("region"),
            col("quantity"),
            col("line_total")
        ).groupBy("year", "month", "year_month", "category", "region") \
        .agg(
            spark_sum("quantity").alias("total_quantity"),
            spark_sum("line_total").alias("total_revenue"),
            count("*").alias("total_transactions"),
            avg("line_total").alias("avg_transaction_value")
        ).orderBy("year", "month", "category", "region")
        
        return monthly_sales
    
    def create_geographic_analytics(self, data: Dict[str, DataFrame]) -> DataFrame:
        """åˆ›å»ºåœ°ç†åˆ†ææ•°æ®"""
        self.logger.info("åˆ›å»ºåœ°ç†åˆ†ææ•°æ®")
        
        customers = data['customers']
        orders = data['orders']
        
        # æŒ‰åœ°åŒºåˆ†æ
        geographic_analytics = customers.join(orders, "customer_id") \
            .groupBy("country", "state", "city", "region") \
            .agg(
                countDistinct("customer_id").alias("unique_customers"),
                count("order_id").alias("total_orders"),
                spark_sum("total_amount").alias("total_revenue"),
                avg("total_amount").alias("avg_order_value")
            ).orderBy(desc("total_revenue"))
        
        return geographic_analytics
    
    def _run_analytics(self, input_path: str, output_path: str, **kwargs) -> ProcessingResult:
        """æ‰§è¡Œç”µå•†æ•°æ®åˆ†æ"""
        try:
            start_time = datetime.now()
            
            # è¯»å–æ•°æ®
            data = self.read_ecommerce_data()
            
            # åˆ›å»ºåˆ†ææ•°æ®
            analytics_results = {}
            
            # å®¢æˆ·åˆ†æ
            customer_analytics = self.create_customer_analytics(data)
            analytics_results['customer_analytics'] = customer_analytics
            self.write_data(
                customer_analytics,
                f"{output_path}/customer_analytics/",
                partition_by=["customer_segment"]
            )
            
            # äº§å“åˆ†æ
            product_analytics = self.create_product_analytics(data)
            analytics_results['product_analytics'] = product_analytics
            self.write_data(
                product_analytics,
                f"{output_path}/product_analytics/",
                partition_by=["category"]
            )
            
            # æœˆåº¦é”€å”®åˆ†æ
            monthly_sales = self.create_monthly_sales_analytics(data)
            analytics_results['monthly_sales'] = monthly_sales
            self.write_data(
                monthly_sales,
                f"{output_path}/monthly_sales/",
                partition_by=["year", "month"]
            )
            
            # åœ°ç†åˆ†æ
            geographic_analytics = self.create_geographic_analytics(data)
            analytics_results['geographic_analytics'] = geographic_analytics
            self.write_data(
                geographic_analytics,
                f"{output_path}/geographic_analytics/",
                partition_by=["country"]
            )
            
            # è®¡ç®—å¤„ç†æŒ‡æ ‡
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            metrics = {
                'processing_time_seconds': processing_time,
                'tables_processed': len(data),
                'analytics_created': len(analytics_results)
            }
            
            # æ·»åŠ æ¯ä¸ªåˆ†æè¡¨çš„è¡Œæ•°
            for name, df in analytics_results.items():
                metrics[f'{name}_row_count'] = df.count()
            
            return ProcessingResult(
                success=True,
                message="ç”µå•†æ•°æ®åˆ†æå®Œæˆ",
                metrics=metrics,
                details={
                    'output_path': output_path,
                    'analytics_types': list(analytics_results.keys())
                }
            )
            
        except Exception as e:
            self.logger.error(f"ç”µå•†æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            return ProcessingResult(
                success=False,
                message="ç”µå•†æ•°æ®åˆ†æå¤±è´¥",
                errors=[str(e)]
            )
    
    def generate_summary_report(self, analytics_path: str) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š")
        
        try:
            report = {}
            
            # è¯»å–å®¢æˆ·åˆ†ææ•°æ®
            customer_analytics = self.read_data(f"{analytics_path}/customer_analytics/", "parquet")
            
            report['customer_summary'] = {
                'total_customers': customer_analytics.count(),
                'active_customers': customer_analytics.filter(col("total_orders") > 0).count(),
                'avg_customer_value': customer_analytics.agg(avg("total_revenue")).collect()[0][0] or 0,
                'top_customer_segment': customer_analytics.groupBy("customer_segment") \
                    .count().orderBy(desc("count")).first()['customer_segment'] if customer_analytics.count() > 0 else None
            }
            
            # è¯»å–äº§å“åˆ†ææ•°æ®
            product_analytics = self.read_data(f"{analytics_path}/product_analytics/", "parquet")
            
            top_product = product_analytics.orderBy(desc("total_revenue")).first()
            report['product_summary'] = {
                'total_products': product_analytics.count(),
                'products_sold': product_analytics.filter(col("total_quantity_sold") > 0).count(),
                'top_selling_product': top_product['product_name'] if top_product else None,
                'avg_product_revenue': product_analytics.agg(avg("total_revenue")).collect()[0][0] or 0
            }
            
            # è¯»å–æœˆåº¦é”€å”®æ•°æ®
            monthly_sales = self.read_data(f"{analytics_path}/monthly_sales/", "parquet")
            
            total_revenue = monthly_sales.agg(spark_sum("total_revenue")).collect()[0][0] or 0
            report['sales_summary'] = {
                'total_revenue': float(total_revenue),
                'total_transactions': monthly_sales.agg(spark_sum("total_transactions")).collect()[0][0] or 0,
                'avg_monthly_revenue': monthly_sales.agg(avg("total_revenue")).collect()[0][0] or 0
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return {'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import json
    
    parser = argparse.ArgumentParser(description='ç”µå•†æ•°æ®åˆ†æå¤„ç†å™¨')
    parser.add_argument('--clean-bucket', required=True, help='æ¸…æ´æ•°æ®S3æ¡¶åç§°')
    parser.add_argument('--analytics-bucket', required=True, help='åˆ†ææ•°æ®S3æ¡¶åç§°')
    parser.add_argument('--output-prefix', default='analytics', help='è¾“å‡ºè·¯å¾„å‰ç¼€')
    parser.add_argument('--generate-report', action='store_true', help='ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š')
    parser.add_argument('--aws-region', default='us-east-1', help='AWSåŒºåŸŸ')
    parser.add_argument('--log-level', default='INFO', help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = DataLakeConfig(
        project_prefix='dl-handson',
        environment='dev',
        aws_region=args.aws_region,
        clean_bucket=args.clean_bucket,
        analytics_bucket=args.analytics_bucket,
        log_level=args.log_level
    )
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        with EcommerceAnalyticsProcessor(config) as processor:
            
            input_path = f"s3a://{args.clean_bucket}/ecommerce/"
            output_path = f"s3a://{args.analytics_bucket}/{args.output_prefix}/"
            
            # æ‰§è¡Œåˆ†æ
            result = processor.process(input_path, output_path)
            
            if result.success:
                print(f"âœ… åˆ†ææˆåŠŸ: {result.message}")
                
                if result.metrics:
                    print(f"ğŸ“Š å¤„ç†æŒ‡æ ‡:")
                    for key, value in result.metrics.items():
                        print(f"  {key}: {value}")
                
                # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
                if args.generate_report:
                    print("ğŸ“‹ ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")
                    report = processor.generate_summary_report(output_path)
                    print(json.dumps(report, indent=2, default=str))
                
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {result.message}")
                for error in result.errors:
                    print(f"é”™è¯¯: {error}")
                sys.exit(1)
                
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()